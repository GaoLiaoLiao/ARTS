# 1. Algorithm

[865. Smallest Subtree with all the Deepest Nodes](https://leetcode.com/problems/smallest-subtree-with-all-the-deepest-nodes/description/)
```Python
思路：
    1. 找到所有最深叶子节点的Lowest Common Ancestor

class Solution:
    def subtreeWithAllDeepest(self, root):
        """
        :type root: TreeNode
        :rtype: TreeNode
        """
        def getHeight(root):
            if root is None:
                return 0
            return max(getHeight(root.left), getHeight(root.right)) + 1
        height = getHeight(root)
        
        def traverse(root, level):
            if root is None:
                return None
            if level == height:
                return root
            
            left = traverse(root.left, level + 1)
            right = traverse(root.right, level + 1)
            
            if left and right:
                return root
            return left if right is None else right
        return traverse(root, 1)
        
        
        


```

# 2. Review

## [How to do distributed locking](https://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html)
This article talks about the potential issues with the Redlock algorithm, which claims to implement fault-tolerant distributed locks on top of Redis.
The author thinks Redis is a good tool and he has successfully used it in production. However, Redis has been gradually misused in areas of data management where there are stronger consistency and durability expectations.

### What are you using that lock for?
When using lock, we use it either for `Efficiency` or `Correctness`. `Efficiency` saves you from doing duplicate operations, while `Correctness` prevents concurrent processes from stepping on each others' toes and messing up the state of your system.
The author thinks that it's not necessary to use Redlock(running 5 redis servers and checking for a majority to acquire your lock). We can just use a single Redis instance with asynchronous replication to a secondary instance in case the primary crashes. It's ok to drop some locks occasionally as the  locks as an efficiency optimization. On the other hand, the Redlock algorithm cannot guarantee correctness for the reasons below. 

### Protecting a resource with a lock
In this part, the author gives an example to illustrate that even a resource is protected by a lock, it's still not safe in some cases, such as `stop-the-world GC pause`. If the GC lasts longer than the lease expiry period, the client doesn't realize that it has expired, it may go ahead and make some unsafe change. This kind of bug cannot be fix by inserting a check on the lock expiry just before writing back to storage as `GC can pause a running thread at any point, including the point that is maximally inconvenient for you(between the last check and the write operation)`.
Reasons why process might get paused:
  1. Stop-the-world GC pause may occur.
  2. Maybe your process tried to read an address that is not yet loaded into memory, so it gets a page fault and is paused until the page is loaded from disk.
  3. Maybe your disk is actually EBS, and so reading a variable unwittingly turned into a synchronous network request over Amazon’s congested network
  4. Maybe there are many other processes contending for CPU, and you hit a black node in your scheduler tree.
  5. Maybe someone accidentally sent SIGSTOP to the process. Whatever. Your processes will get paused.
To fix this problem, a fencing token should be used. A fencing token is a monotonously increasing number given to a client whenever it acquires the lock.
`Zookeeper` is a good lock service to do this: you can use `zxid` or the znode version number as fencing token. However, Redlock doesn't have a mechanism for generating fencing tokens.

### Using time to solve consensus
In this part, the author claims that the most practical system model for this kind of algorithm is the `asynchronous model with unreliable failure detectors`, which makes no assumptions about timing listed below but still do the right thing.
    1. processes may pause for arbitrary lengths of time. 
    2. packets may be arbitrarily delayed in the network. 
    3. clocks may be arbitrarily wrong.
Redlock is not safe as it depends on these timing assumptions: it assumes that all Redis node hold keys for the approximately the right length of time before expiring; that the network delay is small compared to the expiring duration; and that process pauses are much shorter than expiring duration.

### The synchrony assumptions of Redlock
Redlock is correct only if it's being used in a synchronous system model where you have known, fixed upper bound on network delay, pauses and clock drift.
Partially synchronous system means the timing assumptions will be satisfied most of the time and it can be achieved in a well-behaved datacenter environment. However, if you're dependent on Redlock for correctness, "most of the time" is not enough.
Actually, a consensus algorithm has a chance of working for a partially synchronous system model. Raft, Zeb and Paxos all fall in this category. These algorithms must hold no timing assumptions.

### Conclusion
Consensus algorithms should not make timing assumption as it violates safety properties if those assumptions are not met.
If you lock resources only for efficiency, the author recommends sticking with the `straightforward single-node locking algorithm` for Redis.
If you need correctness, don't use Redlock. Instead, use a proper consensus system such as ZooKeeper or use a database with reasonable transaction guarantees. Also, fencing tokens should be used on all resource accesses under the lock.



### Redis is not designed for data management where there are strong consistency and durability expectations.

Before I go into the details of Redlock, let me say that I quite like Redis, and I have successfully used it in production in the past. I think it’s a good fit in situations where you want to share some transient, approximate, fast-changing data between servers, and where it’s not a big deal if you occasionally lose that data for whatever reason. For example, a good use case is maintaining request counters per IP address (for rate limiting purposes) and sets of distinct IP addresses per user ID (for abuse detection).

However, Redis has been gradually making inroads into areas of data management where there are stronger consistency and durability expectations – which worries me, because this is not what Redis is designed for. Arguably, distributed locking is one of those areas. Let’s examine it in some more detail.

It’s important to remember that a lock in a distributed system is not like a mutex in a multi-threaded application. It’s a more complicated beast, due to the problem that different nodes and the network can all fail independently in various ways

This bug is not theoretical: HBase used to have this problem [3,4]. Normally, GC pauses are quite short, but “stop-the-world” GC pauses have sometimes been known to last for several minutes [5] – certainly long enough for a lease to expire.
You cannot fix this problem by inserting a check on the lock expiry just before writing back to storage. However, GC can pause a running thread at any point, including the point that is maximally inconvenient for you (between the last check and the write operation)

### Whatever. Your processes will get paused：
And if you’re feeling smug because your programming language runtime doesn’t have long GC pauses, there are many other reasons why your process might get paused. Maybe your process tried to read an address that is not yet loaded into memory, so it gets a page fault and is paused until the page is loaded from disk. Maybe your disk is actually EBS, and so reading a variable unwittingly turned into a synchronous network request over Amazon’s congested network. Maybe there are many other processes contending for CPU, and you hit a black node in your scheduler tree. Maybe someone accidentally sent SIGSTOP to the process. Whatever. Your processes will get paused.

### Making the lock safe with fencing


### Problems with Redlock
However, this leads us to the first big problem with Redlock: it does not have any facility for generating fencing tokens.


## Two-phase Commit
Two-phase commit, abbreviated to 2PC, is an algorithm for achieving atomic transaction commit across multiple nodes -- either all node commit or all nodes abort. It is commonly being used in distributed databases and also made available to applications in the form of XA transactions.
2PC uses a component called `coordinator`, which can either be implemented as a library, or be a separate process or service.

A 2PC transaction begins with the application reading and writing data on multiple database nodes, as normal. These databases nodes are called `participants` in the transaction. When the application is ready to commit, the coordinator begins phase 1: it sends a prepare request to each of the nodes, asking them whether they are able to commit:</br>
  1. If all participants reply "yes", then the coordinator sends out a commit request to every node in phase 2.
  2. If any of the participants replies "no, the coordinator sends an abort request to all nodes in phase 2.

### A System of Promises
The steps below demonstrate the reason why 2PC works:</br>
  1. When the application wants to begin a distributed transaction, it requests a globally unique transaction ID from the coordinator. 
  2. The application begins a single-node transaction on each of the participants, and attaches the globally unique transaction ID to the single-node transaction. If anything goes wrong at this stage(for example, a node crashes or a request times out), the coordinator or any of the participants can abort.
  3. When the application is ready to commit, the coordinator sends a `prepare request` with the globally unique transaction ID to all participants. `If any of the "prepare requests" fails or times out, the coordinator sends an "abort request" to all nodes to abort the transaction`.
  4. When a participant receives the `prepare request` and before the participant replies "yes" to the coordinator, the participant should make sure that it can definitely commit the transaction under all circumstances. This includes writing all transaction data to disk even it crashed, and checking for any conflicts or constraint violations. In other words, by reply "yes" to the coordinator, the participant promises to commit the transaction without without any error(but the participant hasn't actually committed the transaction at this point of time). 
  5. When the coordinator has received responses from all participants, it makes a definitive decision on whether to commit or abort the transaction according to the participants' responses. The coordinator must write the decision to its `transaction log` on disk so that it knows what decision it made in case it subsequently crashes. This is called the `commit point`.
  6. Once the coordinator's decision has been written to the disk, the commit or abort request will be sent to all participants. `If this request times out, the coordinator must retry forever until it succeeds.` There is no more going back: if the decision was to commit, that decision must be enforced,no matter how many retries it takes. If a participant has crashed in the meantime, the transaction will be committed when it recovers -- since the participant voted "yes", it cannot refuse to commit when it recovers.  

Thus, the protocol contains two crucial "points of no return", which ensures the atomicity of 2PC:</br>
  1. When a participants votes "yes", it promises that it will definitely be able to commit later.
  2. Once the coordinator decides to commit or abort, the decision is irrevocable.

### Coordinator Failure
We have discussed what happens if participants fail or network fails during 2PC:
  1. If any of the `prepare request` fails or times out, the coordinator aborts the transaction.
  2. If any of the `commit request` fails or times out, the coordinator retries forever until it succeeds.
  
But what will happen if coordinator crashes?
If the coordinator fails before sending the prepare request, a participant can safely abort the transaction. But once the participant has received a prepare request and voted "yes", it must wait to hear back from the coordinator whether the transaction was committed or aborted. If the coordinator crashes or the network fails at this point, the participant can do nothing but wait! A participant's transaction in this state is called `in doubt` or `uncertain`.
The only way 2PC can complete is by waiting for the coordinator to recover the in doubt participants. This is why the coordinator must write its commit or abort decision to a transaction log on disk before sending commit or abort request to participants: when the coordinator recovers, it determines the status of all in-doubt transaction by reading its transaction log. Any transactions that don't have a commit record in the coordinator's log are aborted. `Thus, the commit point of 2PC comes down to a regular single-node atomic commit on the coordinator`.

  1. [Designing Data-Intensive Applications - Chapter 9](dataintensive.net)
  2. [Two-Phase Commit - Cloud Computing Concepts](https://www.coursera.org/lecture/cloud-computing-2/2-2-two-phase-commit-5hKqB)
  3. [Two-Phase Commit - COS 418: Distributed Systems](https://www.cs.princeton.edu/courses/archive/fall16/cos418/docs/L6-2pc.pdf)
  4. [关于分布式事务、两阶段提交协议、三阶提交协议](https://www.hollischuang.com/archives/681)


# 3. Tip
## Python: iterable vs iterator
  1. An `iterable` is an object(array, set, tuple, etc. or custom objects) that has an `__iter__()` method which returns an `iterator`, or which defines a `__getitem__()` method that can take sequential indexes starting from zero and raises an `IndexError` when the indexes are no longer valid. In other words, an iterable is an object that you can get an iterator from.
  2. An `iterator` is an object, which uses its `__next__()` method to iterate over an `iterable` object.
  3. Note that every `iterator` is also an `iterable`, but not every iterable is an iterator. For example, a list a iterable but a list is not an iterator.
  4. An iterator can be created from an iterable by using the funtion `iter()`. To make this possible, the class of an object should either implement `__iter__()` method, or implement `__getitem__()` method with sequential indexes starting with 0.
  5. Whenever you use a for loop, or map, or a list comprehension, etc. in Python, the `__next__()` method is called automatically to get each item from the `iterator`.
  6. When a for loop is executed, for statements calls built-in function `iter()` on the object, `which supports the iteration protocol (the __iter__() method)`. If the call succeeds, it will return an iterator object. The iterator defines the `__next__()` method, which accesses elements of the object one at a time. The `__next__()` method will raise a `StopIteration` exception, if there are no further elements available and that's when the for loop terminates.
  7. `__next__()` method can be called using the next() built-in function.

### Reference
  1. [Glossary](https://docs.python.org/3/glossary.html)
  2. [Iterables vs. Iterators vs. Generators(with graph)](https://nvie.com/posts/iterators-vs-generators/)
  3. [Python | Difference between iterable and iterator](https://www.geeksforgeeks.org/python-difference-iterable-iterator/)
  4. [Python __iter__() and __next__() | Converting an object into an iterator](https://www.geeksforgeeks.org/python-__iter__-__next__-converting-object-iterator/)
  5. [What exactly are iterator, iterable, and iteration?](https://stackoverflow.com/questions/9884132/what-exactly-are-iterator-iterable-and-iteration)
  
### Example
```Python
class MyIterator:
    def __init__(self, start, end):
        self.num = start
        self.end = end

    # __iter__ makes an object iterable
    def __iter__(self):
        print("__iter__ CALL")
        return self

    # __next__ makes an object iterator
    def __next__(self):
        if self.num > self.end:
            print("__next__ STOP")
            raise StopIteration
        else:
            print("__next__ CALL")
            self.num += 1
            return self.num - 1

my_iterator1 = MyIterator(3, 4)
for i in my_iterator1:
    print(i)

my_iterator2 = MyIterator(5, 6)
print(next(my_iterator2))
print(next(my_iterator2))


# c = MyIterator(7, 12)
# print(next(c))
# print(next(c))
# iter(c)
# print(next(c))
# print(next(c))
```

# 4. Share
  1. [How I went from being a contributor to an Open Source project maintainer](https://medium.freecodecamp.org/how-i-went-from-being-a-contributor-to-an-open-source-project-maintainer-acd8a6b316f5)</br>
  2. [Things Nobody Told Me About Being a Software Engineer](https://anaulin.org/blog/things-nobody-told-me-about-being-a-software-engineer/?utm_source=wanqu.co&utm_campaign=Wanqu+Daily&utm_medium=website)</br>


  
  
  
  
  
